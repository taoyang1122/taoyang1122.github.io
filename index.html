<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Taojiannan Yang</title>
  
  <meta name="author" content="Taojiannan Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Taojiannan Yang</name>
              </p>
              <!-- <p style="font-size:15px"> -->
              <p> 
                I am an applied scientist at Amazon AGI. Before joining Amazon, I received my PhD degree from the Center for Research in Computer Vision (CRCV), University of Central Florida (UCF), under the supervision of <a href="https://www.crcv.ucf.edu/chenchen/index.html" style="font-size:15px">Prof. Chen Chen</a>. Before that, 
                I received my bachelor's degree from <a href="https://www.ustc.edu.cn/" style="font-size:15px">University of Science and Technology of China (USTC)</a>.
              </p>
              <p style="font-size:15px">
                My current research mainly focuses on diffusion models and multimodal large language models.
              </p>
              <p style="font-size:15px"> My work has been selected as CVPR'22 Best Paper Finalist, and ICCV'25 KnowledgeMR Workshop Best Benchmark Paper. </p>
              <!-- <p style="color:red;"><b> We are hiring research interns! Feel free to drop me an email. </b></p> -->
              <p style="text-align:center">
                <a href="mailto:taojiannan.yang@ucf.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Z_--q5UAAAAJ&hl=en#">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/taoyang1122">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/taojiannan-yang-528706201/">LinkedIn</a> &nbsp/&nbsp
                <!-- <a href="data/CV_website.pdf">CV</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_image.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_image.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/amazon.jpg' width="110">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Applied Scientist</strong>
                <br> Amazon AGI/AWS AI, Aug 2023
                <p> Building <a href="https://aws.amazon.com/ai/generative-ai/nova/">Amazon Nova</a> foundation models.</p> 
              </td>
            </tr>
          </tr>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/aws.jpg' width="110">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Applied Scientist Intern</strong>
                <br> AWS AI Labs, Santa Clara, USA. Summer 2022
                <br> Host: <a href="https://bryanyzhu.github.io/">Yi Zhu</a>,
                <a href="https://www.amazon.science/author/yusheng-xie">Yusheng Xie</a>,
                <a href="https://www.astonzhang.com/">Aston Zhang</a>,
                <a href="https://scholar.google.com/citations?user=Z_WrhK8AAAAJ&hl=en">Mu Li</a>
                <p></p>
                <p> Adapt image models for efficient video understanding.</p>
              </td>
            </tr>
          </tr>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20"></tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/bytedance.jpg' width="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Research Intern</strong>
                <br> ByteDance Inc., Mountain View, USA. Summer 2021
                <br> Host: <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang</a>,
                <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en">Xiaojie Jin</a>
                <p></p>
                <p>Efficient neural architecture search.</p>
              </td>
            </tr>
          </tr>
          </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/scivideobench.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2510.08559">
                <papertitle>SciVideoBench: Benchmarking Scientific Video Reasoning in Large Multimodal Models</papertitle>
              </a>
              <br>
              Andong Deng, 
              <strong> Taojiannan Yang, </strong>
              Shoubin Yu, 
              Lincoln Spencer, 
              Mohit Bansal, 
              Chen Chen, 
              Serena Yeung-Levy,
              Xiaohan Wang
              <br>
              <em><strong><font color="red">Best Benchmark Paper Award at ICCV'25 KnowledgeMR workshop</font></strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2510.08559">paper</a> &nbsp/&nbsp
              <a href="https://github.com/dengandong/SciVideoBench">code</a> &nbsp/&nbsp
              <a href="https://scivideobench.github.io/">leaderboard</a>
              <p></p>
              <p>
                We introduce SciVideoBench, the first scientific video reasning benchmark.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/nova.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card">
                <papertitle>The Amazon Nova Family of Models: Technical Report and Model Card</papertitle>
              </a>
              <br>
              <strong> Core Contributor </strong>
              <br>
              <em>Amazon Science</em>, 2024
              <br>
              <a href="https://assets.amazon.science/10/0a/0b61d39a4e9aaec16f71ad3d9168/the-amazon-nova-family-of-models-technical-report-and-model-card2-26.pdf">paper</a>
              <p></p>
              <p>
                We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/groundMoRe.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2405.13800">
                <papertitle>Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level</papertitle>
              </a>
              <br>
              Andong Den, 
              Tongjia Chen, 
              Shoubin Yu,
              <strong> Taojiannan Yang, </strong>
              Lincoln Spencer,
              Yapeng Tian,
              Ajmal Saeed Mian,
              Mohit Bansal,
              Chen Chen
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2411.09921">paper</a> &nbsp/&nbsp
              <a href="https://groundmore.github.io/">code</a>
              <p></p>
              <p>
                A new Motion-Grounded Video Reasoning benchmark, which evaluates multimodal models‚Äô reasoning and perception capabilities for motion understanding.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/DenseConnector.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2405.13800">
                <papertitle>Dense Connector for MLLMs</papertitle>
              </a>
              <br>
              Huanjin Yao*, 
              Wenhao Wu*, 
              <strong> Taojiannan Yang, </strong>
              Yuxin Song, 
              Mengxi Zhang, 
              Haocheng Feng, 
              Yifan Sun, 
              Zhiheng Li, 
              Wanli Ouyang, 
              Jingdong Wang
              <br>
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2405.13800">paper</a> &nbsp/&nbsp
              <a href="https://github.com/HJYao00/DenseConnector">code</a>
              <p></p>
              <p>
                A universal plug-and-play module to enhance Multimodal-LLM.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()"></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/controlnet++.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2404.07987.pdf">
                <papertitle>ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback</papertitle>
              </a>
              <br>
              Ming Li, 
              <strong> Taojiannan Yang, </strong> 
              Huafeng Kuang, 
              Jie Wu, 
              Zhaoning Wang, 
              Xuefeng Xiao, 
              Chen Chen.
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2404.07987.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/liming-ai/ControlNet_Plus_Plus">code</a> &nbsp/&nbsp
              <a href="https://huggingface.co/spaces/limingcv/ControlNet-Plus-Plus">demo</a>
              <p></p>
              <p>
                Improving the controllability of generative models through discriminative models.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/bear.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.13505.pdf">
                <papertitle>A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition</papertitle>
              </a>
              <br>
              Andong Deng*,
              <strong>Taojiannan Yang*</strong>,
              Chen Chen.
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2303.13505.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/AndongDeng/BEAR">code and data</a>
              <p></p>
              <p>
                A new comprehensive action recognition benchmark to evaluate spatiotemporal representation learning from various perspectives.
              </p>
            </td>
          </tr>
				
          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/AIM.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2302.03024.pdf">
                <papertitle>AIM: Adapting Image Models for Efficient Video Action Recognition</papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Yi Zhu,
              Yusheng Xie,
              Aston Zhang,
              Chen Chen,
              Mu Li.
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2302.03024.pdf">paper</a> &nbsp/&nbsp
              <a href="https://adapt-image-models.github.io/">project</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/adapt-image-models">code</a>
              <p></p>
              <p>
                How to efficiently and effectively adapt image models for video understanding.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/efficientNAS.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2211.08666.pdf">
                <papertitle>Revisiting Training-free NAS Metrics: An Efficient Training-based Method</papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Linjie Yang,
              Xiaojie Jin,
              Chen Chen.
              <br>
              <em>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2211.08666.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/Revisit_TrainingFree_NAS">code</a>
              <p></p>
              <p>
                Training-free metrics are highly correlated with #params and we propose a new efficient training-based method to address the problem.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/fedalign.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2111.14213.pdf">
                <papertitle>Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning</papertitle>
              </a>
              <br>
              Matias Mendieta,
              <strong>Taojiannan Yang</strong>,
              Pu Wang, 
              Minwoo Lee, 
              Zhengming Ding, 
              Chen Chen.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022 
              <br><em><strong><font color="red">(Best Paper Finalist, 33 out of 8161)</font></strong></em>
              <br>
              <a href="https://arxiv.org/pdf/2111.14213.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/mmendiet/FedAlign">code</a>
              <p></p>
              <p>
                GradAug alleviates data heterogeneity in federated learning by smoothing loss landscape. We further improve its efficiency by proposing a new method FedAlign.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/MutualNet-tpami.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2105.07085.pdf">
                <papertitle>MutualNet: Adaptive ConvNet via Mutual Learning from Different Model Configurations</papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Sijie Zhu, 
              Matias Mendieta, 
              Pu Wang, 
              Ravikumar Balakrishnan, 
              Minwoo Lee, 
              Tao Han, 
              Mubarak Shah, 
              Chen Chen.
              <br>
              <em>IEEE Transaction on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2105.07085.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/MutualNet">code</a>
              <p></p>
              <p>
                We extend MutualNet to learn adaptive video models and conduct more analyses.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/gradaug.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2006.07989.pdf">
                <papertitle>GradAug: A New Regularization Method for Deep Neural Networks </papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Sijie Zhu,
              Chen Chen.
              <br>
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2006.07989.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/GradAug">code</a>
              <p></p>
              <p>
                A well-generalized network should make predictions consistent with its subnetworks given differently augmented samples.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/mutualnet.png' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460290.pdf">
                <papertitle>MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution </papertitle>
              </a>
              <br>
              <strong>Taojiannan Yang</strong>,
              Sijie Zhu,
              Chen Chen,
              Shen Yan, 
              Mi Zhang, 
              Andrew Willis.
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
              <br><em><strong><font color="red">(Oral Presentation, 104 out of 5205)</font></strong></em>
              <br>
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460290.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/taoyang1122/MutualNet">code</a>
              <p></p>
              <p>
                We learn networks that can run at different widths and resolutions to meet different resource budegets during runtime.
              </p>
            </td>
          </tr>

          <!-- <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/FedPEFT.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2210.01708.pdf">
                <papertitle>Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning</papertitle>
              </a>
              <br>
              Guangyu Sun, 
              Matias Mendieta,
              <strong>Taojiannan Yang</strong>,
              Chen Chen.
              <br>
              <em>arXiv</em>, 2022
              <br>
              <p></p>
              <p>
                We systematically study the effectiveness of parameter-efficient finetuning techniques in federated learning.
              </p>
            </td>
          </tr>	 -->

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/feater.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2205.15448.pdf">
                <papertitle>FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER</papertitle>
              </a>
              <br>
              Ce Zheng, 
              Matias, Mendieta, 
              <strong>Taojiannan Yang</strong>,
              Guojun Qi, 
              Chen Chen.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2205.15448.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/FeatER">code</a>
              <p></p>
              <p>
                An efficent method for human pose estimation and mesh reconstruction.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/poseformer.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2103.10455.pdf">
                <papertitle>3D Human Pose Estimation with Spatial and Temporal Transformers </papertitle>
              </a>
              <br>
              Ce Zheng, 
              Sijie Zhu, 
              Matias Mendieta,
              <strong>Taojiannan Yang</strong>,
              Chen Chen, 
              Zhengming Ding.
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2103.10455.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/zczcwh/PoseFormer">code</a>
              <p></p>
              <p>
                A spatial-temporal transformer structure for 3D human pose estimation.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/visualmetriclearning.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1909.12977.pdf">
                <papertitle>Visual Explanation for Deep Metric Learning</papertitle>
              </a>
              <br>
              Sijie Zhu,
              <strong>Taojiannan Yang</strong>,
              Chen Chen.
              <br>
              <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/1909.12977.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/Jeff-Zilence/Explain_Metric_Learning">code</a>
              <p></p>
              <p>
                We visualize point-to-point activation intensity between two images.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/vigor.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.12172.pdf">
                <papertitle>VIGOR: Cross-View Image Geo-localization beyond One-to-one Retrieval </papertitle>
              </a>
              <br>
              Sijie Zhu, 
              <strong>Taojiannan Yang</strong>,
              Chen Chen.
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2011.12172.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/Jeff-Zilence/VIGOR">code</a>
              <p></p>
              <p>
                A new benchmark for more realistic cross-view image geo-localization.
              </p>
            </td>
          </tr>

          <tr onmouseout="mira_stop()" onmouseover="mira_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='mira_image'> -->
                  <!-- <img src='images/FedPEFT.JPG' width="160"></div> -->
                <img src='images/longtailuavdetection.JPG' width="160">
              </div>
              <script type="text/javascript">
                function mira_start() {
                  document.getElementById('mira_image').style.opacity = "1";
                }

                function mira_stop() {
                  document.getElementById('mira_image').style.opacity = "0";
                }
                mira_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2011.03822.pdf">
                <papertitle>Towards Resolving the Challenge of Long-tail Distribution in UAV Images for Object Detection </papertitle>
              </a>
              <br>
              Weiping Yu*,
              <strong>Taojiannan Yang*</strong>,
              Chen Chen.
              <br>
              <em>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/pdf/2011.03822.pdf">paper</a> &nbsp/&nbsp
              <a href="https://github.com/we1pingyu/DSHNet">code</a>
              <p></p>
              <p>
                We point out the long-tail distribution problem in UAV images and propose a new method to address it.
              </p>
            </td>
          </tr>
				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, IEEE Transaction on Pattern Analysis and Machine Intelligence</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, IEEE Transaction on Image Processing</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, CVPR 2022, 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ECCV 2022</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICCV 2021, 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, NeurIPS 2021, 2022</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICLR 2022, 2023</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Reviewer, ICML 2022</a>
              <br>
              <a href="https://cvpr2022.thecvf.com/area-chairs">Volunteer, NeurIPS 2020</a>
            </td>
          </tr>

		
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
